# reproduce
seed: 2026

# model
vocab_size: 16 # 32
d_model: 32 # 64
ff_dim: 256
num_heads: 1
num_layers: 2

# TF model variants
linear_attn: False
residual: True
mlp: False
dropout: 0.1
norm: True
output_norm: False
trainable_norm: False
pos: "rotary"
rotary_theta: 10000

# data generation
max_seq_len: 64
sample_size: 5000
sample_size_test: 5000
regime: "varied repetition" # "varied repetition"
distr: "two-level"
rep_l: 10
rep_h: 20
ood_len_pattern: 25
pool_size: null
sig: 2

# training
device: "cuda"
fancy_opt: False
use_wd: True
schedule: "constant"
fresh_sample: True
label_smoothing: False
optimizer: "sam"
lr: 0.001 # 0.001
wd: 0.0005
batch_size: 50
num_epoch: 20000 # 10000
sam_rho: 0.2

# logging
wandb_log: False
plot_attn_every_epoch: 10000
print_output: False
n_save: 1 # 500
up_to_first_save: False

# eval
ignore_segment: 1
ignore_burning: 4

# IO
out_dir: "out"

# sharpness task
sharpness_step: 500
sharpness_task: "Hessian-trace" # "outer-product-Hessian-random-alignment"